include::./attributes.adoc[]
= {project-name} - Calling openfaas methods

:toc: macro
:toclevels: 4
:doctype: book
:icons: font
:docinfo1:

:numbered:
:sectnums:
:sectnumlevels: 4


Learn how to communicate with an openfaas server to execute some custom code of your company.

This guide covers:

* rest service
* executing request against a faas server and retrieving the response

== Prerequisites

To complete this guide, you need:

* less than 15 minutes
* an IDE
* JDK 1.8+ installed with `JAVA_HOME` configured appropriately
* Apache Maven 3.5.3+
* a running openfaas server available from your machine. (See openfaas documentation to install one)
* to be familiar with logisland (please refer to other guides first otherwise)

== Setup openfaas uppercase function

We built a custom method that returns request body in uppercase. You will have to deploy it to your openfaas server to follow this tutorial.
You can use your own function if you wish but you may have to adapt the construction of the request (the logisland conf job) accordingly.

TODO

== Solution

We recommend that you follow the instructions in the next sections and create the application step by step.
However, you can go right to the completed example.

* Clone the Git repository: `git clone {quickstarts-clone-url}`, or download an {quickstarts-archive-url}[archive].
* The solution is located in the `conf/rest/uppercase_input_with_openfaas.yml` file.

== Logisland job setup

1. We will parse some apache logs
2. Then use those record as body to a custom openfaas method that returns request body's in uppercase (body's request is expected to be text). Retrieve this response in a field
    Anyway as of today the rest service only support body as string.
3. Extract the response from the root record to keep only response
4. We will verify that output records in kafka are in uppercase.

[NOTE]

Notice that you will have to replace
`rest.lookup.url: http://192.168.99.100:31112/function/${function_name}`
with the endpoint of your openfaas server !

[source,yaml,subs=attributes+]
----
#########################################################################################################
# Logisland configuration script template
#########################################################################################################

version: 1.2.0
documentation: LogIsland main config file. Put here every engine or component config

#########################################################################################################
# engine
engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  type: engine
  documentation: A logisland stream that convert all string to uppercase using an openfaas method
  configuration:
    spark.app.name: OpenFaasDemo
    spark.master: local[4]
    spark.driver.memory: 1g
    spark.driver.cores: 1
    spark.executor.memory: 2g
    spark.executor.instances: 4
    spark.executor.cores: 2
    spark.yarn.queue: default
    spark.yarn.maxAppAttempts: 4
    spark.yarn.am.attemptFailuresValidityInterval: 1h
    spark.yarn.max.executor.failures: 20
    spark.yarn.executor.failuresValidityInterval: 1h
    spark.task.maxFailures: 8
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    spark.streaming.batchDuration: 10000
    spark.streaming.backpressure.enabled: false
    spark.streaming.unpersist: false
    spark.streaming.blockInterval: 500
    spark.streaming.kafka.maxRatePerPartition: 3000
    spark.streaming.timeout: -1
    spark.streaming.kafka.maxRetries: 3
    spark.streaming.ui.retainedBatches: 200
    spark.streaming.receiver.writeAheadLog.enable: false
    spark.ui.port: 4053

  controllerServiceConfigurations:

    - controllerService: faas_service_rest
      component: com.hurence.logisland.rest.service.lookup.RestLookupService
      configuration:
        rest.lookup.url: http://192.168.99.100:31112/function/${function_name}
        record.serializer: com.hurence.logisland.serializer.ExtendedJsonSerializer
        #record.schema: ff
        rest.lookup.basic.auth.username: admin
        rest.lookup.basic.auth.password: 4f1c1b3f032b635cf1a4b1cb3d9a514412d0b7ab
        rest.lookup.digest.auth: false
        #eventuellement SSL config dans un second temps

  streamConfigurations:

    - stream: parsing_stream
      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
      configuration:
        kafka.input.topics: logisland_raw
        kafka.output.topics: logisland_events
        kafka.error.topics: logisland_errors
        kafka.input.topics.serializer: none
        kafka.output.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: kafka:9092
        kafka.zookeeper.quorum: zookeeper:2181
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 4
        kafka.topic.default.replicationFactor: 1
      processorConfigurations:

        # a parser that produce events from an apache log REGEX
        - processor: apache_parser
          component: com.hurence.logisland.processor.SplitText
          configuration:
            record.type: apache_log
            value.regex: (\S+)\s+(\S+)\s+(\S+)\s+\[([\w:\/]+\s[+\-]\d{4})\]\s+"(\S+)\s+(\S+)\s*(\S*)"\s+(\S+)\s+(\S+)
            value.fields: src_ip,identd,user,record_time,http_method,http_query,http_version,http_status,bytes_out
        # add field for openfaas method to use
        - processor: add_function_name
          component: com.hurence.logisland.processor.AddFields
          configuration:
            function_name: uppercase
        # add field if defect found
        - processor: defect_tagger
          component: com.hurence.logisland.rest.processor.lookup.CallRequest
          configuration:
            http.client.service: faas_service_rest
            strategy: overwrite_existing
            field.http.response: copy_to_uppercase
            request.method: "POST"
            request.mime.type: "application/json"
            input.as.body: true
        - processor: flatten
          component: com.hurence.logisland.processor.FlatMap
          documentation: "extract response from root record and remove root record"
          configuration:
            keep.root.record: false
            copy.root.record.fields: false
            leaf.record.type: http_response
----


== Setting up Underlyning services
Apache Kafka is pitched as a Distributed Streaming Platform. In Kafka lingo, Producers continuously generate data (streams) and Consumers are responsible for processing, storing and analysing it. Kafka Brokers are responsible for ensuring that in a distributed scenario the data can reach from Producers to Consumers without any inconsistency. A set of Kafka brokers and another piece of software called zookeeper constitute a typical Kafka deployment.

Let’s start with a single broker instance. In the `logisland-quickstarts` directory create your `docker-compose.yml` with the following content:

[source,yml,subs=attributes+]
----
version: '3'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
    networks:
      - logisland

  kafka:
    image: wurstmeister/kafka:0.10.2.1
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_CREATE_TOPICS: "test:1:1"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./data:/tmp/data
    networks:
      - logisland

  logisland:
    image: hurence/logisland:latest
    entrypoint:
      - "tail"
      - "-f"
      - "bin/logisland.sh"
    ports:
      - '4050:4050'
    volumes:
      - ./conf:/opt/logisland/conf
    environment:
      KAFKA_HOME: /opt/kafka_2.11-0.10.2.2
      KAFKA_BROKERS: kafka:9092
      ZK_QUORUM: zookeeper:2181
      REDIS_CONNECTION: redis:6379
    networks:
      - logisland

  loggen:
    image: hurence/loggen:latest
    networks:
      - logisland
    environment:
      LOGGEN_NUM: 1
      KAFKA_BROKERS: kafka:9092

  redis:
    hostname: redis
    image: 'redis:latest'
    ports:
      - '6379:6379'
    networks:
      - logisland

volumes:
  kafka-home:

networks:
  logisland:
----

Be carefull here to not generate too many log if your openfaas can not follow up ! Here I run a single container for my function so I set up loggen this way :

----
[source,yml,subs=attributes+]
  loggen:
    image: hurence/loggen:latest
    networks:
      - logisland
    environment:
      LOGGEN_NUM: 1
      KAFKA_BROKERS: kafka:9092
----

Start Zookeeper, Kafka, Loggen & Logisland container with the following command `docker-compose up -d`

== Launch the script

Connect a shell to your logisland container to launch the following streaming jobs.

`docker exec -i -t logisland-quickstarts_logisland_1 bin/logisland.sh --conf conf/rest/uppercase_input_with_openfaas.yml`


=== See what we have in return

Logisland handles the parsing of the log lines and structure them as Records before sending them to `logisland_events`. Those will be sent in another Kafka topic.

`docker exec -ti logisland-quickstarts_kafka_1 /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic logisland_events`


You should see the apache log events such as :

[source,shell]
----
{
  "id" : "e363889d-a00c-4084-b733-e8eb6dc759ac",
  "type" : "http_response",
  "creationDate" : 1570028572284,
  "fields" : {
    "RECORD_VALUE" : "123.124.125.126 - - [02/OCT/2019:15:02:38 +0200] \"GET /WP-CONTENT HTTP/1.0\" 200 5034",
    "CREATIONDATE" : 1570021358000,
    "IDENTD" : "-",
    "RECORD_TIME" : 1570021358000,
    "HTTP_STATUS" : "200",
    "SRC_IP" : "123.124.125.126",
    "HTTP_VERSION" : "HTTP/1.0",
    "USER" : "-",
    "record_type" : "http_response",
    "BYTES_OUT" : "5034",
    "record_id" : "e363889d-a00c-4084-b733-e8eb6dc759ac",
    "FUNCTION_NAME" : "UPPERCASE",
    "RECORD_TYPE" : "APACHE_LOG",
    "RECORD_ID" : "A0CABEBA-27F7-45F4-B484-55FA8C555CE1",
    "HTTP_QUERY" : "/WP-CONTENT",
    "ID" : "A0CABEBA-27F7-45F4-B484-55FA8C555CE1",
    "record_time" : 1570028572284,
    "TYPE" : "APACHE_LOG",
    "HTTP_METHOD" : "GET"
  }
}
----

On constate que tous les champs sont bien en uppercase. A l'exception des champs techniques et des champs rajouté par le processor FlatMap.
